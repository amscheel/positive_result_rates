
@article{Agnoli2017,
  title = {Questionable Research Practices among Italian Research Psychologists},
  author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Albiero, Paolo and Cubelli, Roberto},
  year = {2017},
  volume = {12},
  pages = {e0172792},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0172792},
  abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88\%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
  bdsk-url-2 = {http://dx.doi.org/10.1371/journal.pone.0172792},
  journal = {PLOS ONE},
  number = {3}
}

@article{Allen2019,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  author = {Allen, Christopher and Mehler, David M. A.},
  year = {2019},
  month = may,
  volume = {17},
  pages = {e3000246},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  journal = {PLOS Biology},
  language = {en},
  number = {5}
}

@book{AssociationforPsychologicalScience,
  title = {Registered {{Replication Reports}}},
  author = {{Association for Psychological Science}}
}

@article{Atkinson1982,
  title = {Statistical Significance, Reviewer Evaluations, and the Scientific Process: {{Is}} There a (Statistically) Significant Relationship?},
  shorttitle = {Statistical Significance, Reviewer Evaluations, and the Scientific Process},
  author = {Atkinson, Donald R. and Furlong, Michael J. and Wampold, Bruce E.},
  year = {1982},
  volume = {29},
  pages = {189--194},
  issn = {0022-0167},
  doi = {10.1037/0022-0167.29.2.189},
  journal = {Journal of Counseling Psychology},
  language = {en},
  number = {2}
}

@article{Bakker2018,
  title = {Ensuring the Quality and Specificity of Preregistrations},
  author = {Bakker, Marjan and Veldkamp, Coosje Lisabet Sterre and van Assen, Marcel A. L. M. and Crompvoets, Elise Anne Victoire and Ong, How Hwee and Nosek, Brian A. and Soderberg, Courtney K. and Mellor, David Thomas and Wicherts, Jelte},
  year = {2018},
  month = sep,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/cdgyh},
  abstract = {Researchers face many, often seemingly arbitrary choices in formulating hypotheses, designing protocols, collecting data, analyzing data, and reporting results. Opportunistic use of `researcher degrees of freedom' aimed at obtaining statistical significance increases the likelihood of obtaining and publishing false positive results and overestimated effect sizes. Preregistration is a mechanism for reducing such degrees of freedom by specifying designs and analysis plans before observing the research outcomes. The effectiveness of preregistration may depend, in part, on whether the process facilitates sufficiently specific articulation of such plans. In this preregistered study, we compared two formats of preregistration available on the OSF: Standard Pre-Data Collection Registration and Prereg Challenge registration (now called ``OSF Preregistration'', http://osf.io/prereg/). The Prereg Challenge format was a structured workflow with detailed instructions, and an independent review to confirm completeness; the ``Standard'' format was unstructured with minimal direct guidance to give researchers flexibility for what to pre-specify. Results of comparing random samples of 53 preregistrations from each format indicate that the structured format restricted the opportunistic use of researcher degrees of freedom better (Cliff's Delta = 0.49) than the unstructured format, but neither eliminated all researcher degrees of freedom. We also observed very low concordance among coders about the number of hypotheses (14\%), indicating that they are often not clearly stated. We conclude that effective preregistration is challenging, and registration formats that provide effective guidance may improve the quality of research.},
  keywords = {Meta-science,preregistration,Quantitative Methods,Questionable research practices,researcher degrees of freedom,Social and Behavioral Sciences,Statistical Methods}
}

@article{Baumeister2016,
  title = {Charting the Future of Social Psychology on Stormy Seas: {{Winners}}, Losers, and Recommendations},
  shorttitle = {Charting the Future of Social Psychology on Stormy Seas},
  author = {Baumeister, Roy F.},
  year = {2016},
  month = sep,
  volume = {66},
  pages = {153--158},
  issn = {00221031},
  doi = {10.1016/j.jesp.2016.02.003},
  journal = {Journal of Experimental Social Psychology},
  language = {en}
}

@book{CenterforOpenSciencea,
  title = {{{OSF}} - {{Registered Reports}}},
  author = {{Center for Open Science}}
}

@article{Chambers2013,
  title = {Registered Reports: A New Publishing Initiative at {{Cortex}}},
  author = {Chambers, C.D.},
  year = {2013},
  volume = {49},
  pages = {609--610},
  number = {3}
}

@article{Chambers2015,
  title = {Registered {{Reports}}: {{Realigning}} Incentives in Scientific Publishing},
  author = {Chambers, C.D. and Dienes, Z. and McIntosh, R.D. and Rotshtein, P. and Willmes, K.},
  year = {2015},
  volume = {66},
  pages = {1--2},
  issn = {19738102},
  doi = {10.1016/j.cortex.2015.03.022},
  abstract = {This editorial present views on realigning incentives in scientific publishing. As editors recognize this important moment for Cortex, they also take the opportunity to reiterate our view that Registered Reports should not be seen as a one-shot cure for reproducibility problems in science. The applicability of Registered Reports to different sub-fields within neuropsychology and cognitive neuroscience remains to be established; for instance, studies that rely exclusively on exploration rather than deductive hypothesis testing may not be compatible. Registered Reports present no threat to exploratory science in cases where studies include a mixture of both hypothesis testing and exploratory analyses, authors are welcome to report the outcomes of the unregistered analyses, as Sassenhagen and Bornkessel-Schlesewsky do in the current issue. Pre-registration simply allows readers to distinguish the outcomes based on a priori hypothesis testing from post hoc exploration. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Cortex},
  pmid = {25892410}
}

@techreport{Chambers2020,
  title = {Registered {{Reports}}: {{Past}}, {{Present}} and {{Future}}},
  shorttitle = {Registered {{Reports}}},
  author = {Chambers, Christopher D and Tzavella, Loukia},
  year = {2020},
  month = feb,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/43298},
  abstract = {Registered Reports are a form of empirical journal article in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory, and proposed methods, Registered Reports offer a powerful remedy for a range of reporting and publication biases. Here we reflect on the history, progress and future prospects of the Registered Reports initiative, and also offer practical guidance for authors, reviewers, and editors encountering the format for the first time. While the key ingredients of pre-study review and results-blind acceptance are far from novel \textendash{} and are already adopted independently in a variety of contexts \textendash{} Registered Reports are the first mechanism to combine them into a mainstream policy that has won appeal with multiple stakeholders in the research process. We review early evidence that Registered Reports are working as intended, while at the same acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in future, to address limitations and adapt to new challenges. In spite of these caveats, we conclude that Registered Reports are promoting reproducibility, transparency and self-correction across a wide range of disciplines, and may help reshape how society evaluates research and researchers.},
  type = {Preprint}
}

@article{Cleophas1999,
  title = {Is Selective Reporting of Clinical Research Unethical as Well as Unscientific?},
  author = {Cleophas, R. C. and Cleophas, T. J.},
  year = {1999},
  month = jan,
  volume = {37},
  pages = {1--7},
  issn = {0946-1965},
  abstract = {BACKGROUND: Studies that do not confirm their prior hypotheses, otherwise called "negative" studies, receive less interest from different parties including authors, editors and sponsors, and so, not to publish such studies is a common phenomenon. Opinions differ on whether or not this phenomenon introduces imprecision into the assessment of health research and care. OBJECTIVE: The current paper gives arguments against and in favor of publishing "negative" trials, and tries to give suggestions for a more balanced approach to this problem. RESULTS: Arguments against publishing "negative" trials include: we need not publish erroneously "negative" trials; we need not publish a "negative" study out of worry that the favored treatment is inferior; full-length reports of "negative" trials devaluate the quality of literature, because the data are usually not so important, and generally receive little interest from readers, and so, not to publish them is a more or less "natural" matter of course. Arguments in favor of publishing "negative" trials include: no report reduces the flow of information because "negative" trials provide at least some evidence and balance against the overwhelming power of positive data readily accepted for publication; no report violates the promise to patient participants; studies that do not confirm prior hypotheses are especially important; not-publishing leads to unnecessary repetition of research. Initially, trials were frequently "negative" not only due to lack of power but also due to inappropriate hypotheses and poor designs. Currently, this is less so, and the issue of selective reporting, therefore, needs to be reassessed. Suggestions for a more balanced approach to the problem of selective reporting might include: careful planning before the trial begins, reduces the chance of biased and erroneously "negative" trials; any trial, "positive" or "negative", provides probabilities rather than truths; this notion does not explain away publication bias but does make it less of a problem; "negative" trials may not be appropriate for general journals but very relevant to specialist journals as well as other organs of specialist groups; ethical committees and trial review boards should address the issue of publishing as part of their function. CONCLUSION: Data from properly executed trials should routinely be made available. However, we should not forget that the empirical observations provided by clinical trials, are statistically tested, and that statistics are based merely on probabilities. It means that we must consider a more philosophical attitude to clinical trial evidence in terms of acceptance that scientific truths are rarely absolute.},
  journal = {International Journal of Clinical Pharmacology and Therapeutics},
  keywords = {Clinical Trials as Topic,Ethics; Medical,Humans,Publication Bias,Research,Treatment Failure},
  language = {eng},
  number = {1},
  pmid = {10027477}
}

@article{Cristea2018,
  title = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant: {{A}} Survey of Top Science Journals},
  shorttitle = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant},
  author = {Cristea, Ioana Alina and Ioannidis, John P. A.},
  year = {2018},
  month = may,
  volume = {13},
  pages = {e0197440},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0197440},
  abstract = {P values represent a widely used, but pervasively misunderstood and fiercely contested method of scientific inference. Display items, such as figures and tables, often containing the main results, are an important source of P values. We conducted a survey comparing the overall use of P values and the occurrence of significant P values in display items of a sample of articles in the three top multidisciplinary journals (Nature, Science, PNAS) in 2017 and, respectively, in 1997. We also examined the reporting of multiplicity corrections and its potential influence on the proportion of statistically significant P values. Our findings demonstrated substantial and growing reliance on P values in display items, with increases of 2.5 to 14.5 times in 2017 compared to 1997. The overwhelming majority of P values (94\%, 95\% confidence interval [CI] 92\% to 96\%) were statistically significant. Methods to adjust for multiplicity were almost non-existent in 1997, but reported in many articles relying on P values in 2017 (Nature 68\%, Science 48\%, PNAS 38\%). In their absence, almost all reported P values were statistically significant (98\%, 95\% CI 96\% to 99\%). Conversely, when any multiplicity corrections were described, 88\% (95\% CI 82\% to 93\%) of reported P values were statistically significant. Use of Bayesian methods was scant (2.5\%) and rarely (0.7\%) articles relied exclusively on Bayesian statistics. Overall, wider appreciation of the need for multiplicity corrections is a welcome evolution, but the rapid growth of reliance on P values and implausibly high rates of reported statistical significance are worrisome.},
  journal = {PLOS ONE},
  keywords = {Analysis of variance,Bayesian method,Bayesian statistics,Computer software,Meta-analysis,Scientific publishing,Software tools,Statistical data},
  language = {en},
  number = {5}
}

@article{deWinter2013,
  title = {Why {{Selective Publication}} of {{Statistically Significant Results Can Be Effective}}},
  author = {{de Winter}, Joost and Happee, Riender},
  year = {2013},
  month = jun,
  volume = {8},
  pages = {e66463},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0066463},
  abstract = {Concerns exist within the medical and psychological sciences that many published research findings are not replicable. Guidelines accordingly recommend that the file drawer effect should be eliminated and that statistical significance should not be a criterion in the decision to submit and publish scientific results. By means of a simulation study, we show that selectively publishing effects that differ significantly from the cumulative meta-analytic effect evokes the Proteus phenomenon of poorly replicable and alternating findings. However, the simulation also shows that the selective publication approach yields a scientific record that is content rich as compared to publishing everything, in the sense that fewer publications are needed for obtaining an accurate meta-analytic estimation of the true effect. We conclude that, under the assumption of self-correcting science, the file drawer effect can be beneficial for the scientific collective.},
  journal = {PLOS ONE},
  keywords = {Heart,Medicine and health sciences,Mental health and psychiatry,Meta-analysis,meta-science,power,publication bias,Replication studies,Research design,Research validity,Scientific publishing,stats},
  language = {en},
  number = {6}
}

@article{Fanelli2010,
  title = {"{{Positive}}" Results Increase down the Hierarchy of the Sciences},
  author = {Fanelli, Daniele},
  editor = {Scalas, Enrico},
  year = {2010},
  month = apr,
  volume = {5},
  pages = {e10068},
  issn = {19326203},
  doi = {10.1371/journal.pone.0010068},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the "hardness" of scientific research\textendash i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors\textendash is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a "positive" (full or partial) or "negative" support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in "softer" sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  journal = {PLoS ONE},
  number = {4},
  pmid = {20383332}
}

@article{Fanelli2012,
  title = {Negative Results Are Disappearing from Most Disciplines and Countries},
  author = {Fanelli, Daniele},
  year = {2012},
  month = mar,
  volume = {90},
  pages = {891--904},
  issn = {01389130},
  doi = {10.1007/s11192-011-0494-7},
  abstract = {Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have ``tested'' a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22\% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.},
  journal = {Scientometrics},
  keywords = {Bias,Competition,Misconduct,Publication,Publish or perish,Research evaluation},
  number = {3}
}

@article{Ferguson2012,
  title = {A {{Vast Graveyard}} of {{Undead Theories}}: {{Publication Bias}} and {{Psychological Science}}'s {{Aversion}} to the {{Null}}},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  year = {2012},
  volume = {7},
  pages = {555--561},
  issn = {17456916},
  doi = {10.1177/1745691612459059},
  abstract = {Publication bias remains a controversial issue in psychological science. The tendency of psychological science to avoid publishing null results produces a situation that limits the replicability assumption of science, as replication cannot be meaningful without the potential acknowledgment of failed replications. We argue that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science's capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous ``undead'' theories that are ideologically popular but have little basis in fact.},
  journal = {Perspectives on Psychological Science},
  keywords = {fail-safe number,falsification,meta-analyses,null hypothesis significance testing,publication bias},
  number = {6},
  pmid = {26168112}
}

@article{Fiedler2016,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  year = {2016},
  month = jan,
  volume = {7},
  pages = {45--52},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615612150},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  journal = {Social Psychological and Personality Science},
  language = {en},
  number = {1}
}

@article{Franco2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, A. and Malhotra, N. and Simonovits, G.},
  year = {2014},
  volume = {345},
  pages = {1502--1505},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1255484},
  journal = {Science},
  keywords = {publication bias,replication crisis},
  language = {en},
  number = {6203}
}

@article{Franco2016,
  title = {Underreporting in {{Psychology Experiments}}: {{Evidence From}} a {{Study Registry}}},
  shorttitle = {Underreporting in {{Psychology Experiments}}},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  year = {2016},
  volume = {7},
  pages = {8--12},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615598377},
  journal = {Social Psychological and Personality Science},
  keywords = {publication bias,replication crisis},
  language = {en},
  number = {1}
}

@article{Fraser2018,
  title = {Questionable Research Practices in Ecology and Evolution},
  author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
  year = {2018},
  month = jul,
  volume = {13},
  pages = {e0200303},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0200303},
  abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
  journal = {PLOS ONE},
  keywords = {Behavioral ecology,Community ecology,Evolutionary biology,Evolutionary ecology,Evolutionary rate,Psychology,Publication ethics,Statistical data},
  language = {en},
  number = {7}
}

@misc{Goldacre2016,
  title = {The {{COMPare Trials Project}}},
  author = {Goldacre, Ben and Drysdale, Henry and {Powell-Smith}, Anna and Dale, Aaron and Milosevic, Ioan and Slade, Eirion and Hartley, Philip and Marston, Cicely and Mahtani, Kamal and Heneghan, Carl},
  year = {2016},
  abstract = {Outcome switching in clinical trials is a serious problem. Between October 2015 and January 2016, the COMPare team systematically checked every trial published in the top five medical journals, to see if they misreported their findings. We are now submitting the first set of findings from the projec},
  url = {http://compare-trials.org},
  journal = {COMPare},
  language = {en-GB}
}

@article{Greenwald1975,
  title = {Consequences of {{Prejudice Against}} the {{Null Hypothesis}}},
  author = {Greenwald, Anthony G.},
  year = {1975},
  volume = {82},
  pages = {1--20},
  journal = {Psychological Bulletin},
  number = {1}
}

@article{Hardwicke2018,
  title = {Mapping the Universe of Registered Reports},
  author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
  year = {2018},
  month = oct,
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0444-y},
  journal = {Nature Human Behaviour},
  keywords = {journal policy,meta-research,open science,pre-registration,Registered Reports}
}

@article{John2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  volume = {23},
  pages = {524--532},
  issn = {14679280},
  doi = {10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  journal = {Psychological Science},
  keywords = {disclosure,judgment,methodology,professional standards},
  number = {5},
  pmid = {22508865}
}

@article{Jonas2016,
  title = {How Can Preregistration Contribute to Research in Our Field?},
  author = {Jonas, Kai J. and Cesario, Joseph},
  year = {2016},
  volume = {1},
  pages = {1--7},
  issn = {2374-3603, 2374-3611},
  doi = {10.1080/23743603.2015.1070611},
  journal = {Comprehensive Results in Social Psychology},
  language = {en},
  number = {1-3}
}

@article{Kaplan2015,
  title = {Likelihood of {{Null Effects}} of {{Large NHLBI Clinical Trials Has Increased}} over {{Time}}},
  author = {Kaplan, Robert M. and Irvin, Veronica L.},
  editor = {Garattini, Silvio},
  year = {2015},
  month = aug,
  volume = {10},
  pages = {e0132382},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0132382},
  journal = {PLOS ONE},
  language = {en},
  number = {8}
}

@article{Kerr1998,
  title = {{{HARKing}}: {{Hypothesizing}} after the Results Are Known},
  author = {Kerr, Norbert L.},
  year = {1998},
  month = aug,
  volume = {2},
  pages = {196--217},
  issn = {10888683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  journal = {Personality and Social Psychology Review},
  number = {3},
  pmid = {15647155}
}

@article{Kohler2019,
  title = {Play {{It Again}}, {{Sam}}! {{An Analysis}} of {{Constructive Replication}} in the {{Organizational Sciences}}},
  author = {K{\"o}hler, Tine and Cortina, Jose M.},
  year = {2019},
  month = apr,
  pages = {014920631984398},
  issn = {0149-2063, 1557-1211},
  doi = {10.1177/0149206319843985},
  journal = {Journal of Management},
  language = {en}
}

@article{Lakens2018a,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  volume = {1},
  pages = {259--269},
  issn = {10959513},
  doi = {10.1177/2515245918770963},
  abstract = {The North American carnivorous pitcher plant genus Sarracenia (Sarraceniaceae) is a relatively young clade (\textbackslash textless3 million years ago) displaying a wide range of morphological diversity in complex trapping structures. This recently radiated group is a promising system to examine the structural evolution and diversification of carnivorous plants; however, little is known regarding evolutionary relationships within the genus. Previous attempts at resolving the phylogeny have been unsuccessful, most likely due to few parsimony-informative sites compounded by incomplete lineage sorting. Here, we applied a target enrichment approach using multiple accessions to assess the relationships of Sarracenia species. This resulted in 199 nuclear genes from 75 accessions covering the putative 8-11 species and 8 subspecies/varieties. In addition, we recovered 42. kb of plastome sequence from each accession to estimate a cpDNA-derived phylogeny. Unsurprisingly, the cpDNA had few parsimony-informative sites (0.5\%) and provided little information on species relationships. In contrast, use of the targeted nuclear loci in concatenation and coalescent frameworks elucidated many relationships within Sarracenia even with high heterogeneity among gene trees. Results were largely consistent for both concatenation and coalescent approaches. The only major disagreement was with the placement of the purpurea complex. Moreover, results suggest an Appalachian massif biogeographic origin of the genus. Overall, this study highlights the utility of target enrichment using multiple accessions to resolve relationships in recently radiated taxa.},
  journal = {Advances in Methods and Practices in Psychological Science},
  keywords = {equivalence testing,falsification,frequentist,null hypothesis,null-hypothesis significance test,open,power,tost},
  number = {2},
  pmid = {25689607}
}

@article{Lakens2019,
  title = {{The value of preregistration for psychological science: A conceptual analysis}},
  shorttitle = {{The value of preregistration for psychological science}},
  author = {Lakens, Dani{\"e}l},
  year = {2019},
  volume = {62},
  pages = {221--230},
  publisher = {{心理学評論刊行会}},
  issn = {0386-1058, 2433-4650},
  doi = {10.24602/sjpr.62.3_221},
  abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish  \ldots},
  journal = {Japanese Psychological Review},
  language = {ja},
  number = {3}
}

@article{Mahoney1977,
  title = {Publication {{Prejudices}}: {{An Experimental Study}} of {{Confirmatory Bias}} in the {{Peer Review System}}},
  author = {Mahoney, Michael J.},
  year = {1977},
  volume = {1},
  pages = {161--175},
  doi = {10.1007/BF01173636},
  journal = {Cognitive Therapy and Research},
  number = {2}
}

@article{Makel2012,
  title = {Replications in {{Psychology Research}}: {{How Often Do They Really Occur}}?},
  shorttitle = {Replications in {{Psychology Research}}},
  author = {Makel, Matthew C. and Plucker, Jonathan A. and Hegarty, Boyd},
  year = {2012},
  month = nov,
  doi = {10.1177/1745691612460688},
  abstract = {Recent controversies in psychology have spurred conversations about the nature and quality of psychological research. One topic receiving substantial attention ...},
  journal = {Perspectives on Psychological Science},
  keywords = {meta-science,replication},
  language = {en}
}

@article{Makel2019,
  title = {Questionable and {{Open Research Practices}} in {{Education Research}}},
  author = {Makel, Matthew C. and Hodges, Jaret and Cook, Bryan G. and Plucker, Jonathan},
  year = {2019},
  month = oct,
  publisher = {{EdArXiv}},
  doi = {10.35542/osf.io/f7srb},
  abstract = {Discussions of how to improve research quality are predominant in a number of fields, including education. But how prevalent are the use of problematic practices and the improved practices meant to counter them? This baseline information will be a critical data source as education researchers seek to improve our research practices. In this preregistered study, we replicated and extended previous studies from other fields by asking education researchers about 10 questionable research practices and 5 open research practices. We asked them to estimate the prevalence of the practices in the field, self-report their own use of such practices, and estimate the appropriateness of these behaviors in education research. We made predictions under four umbrella categories: comparison to psychology, geographic location, career stage, and quantitative orientation. Broadly, our results suggest that both questionable and open research practices are part of the typical research practices of many educational researchers. Preregistration, code, and data can be found at https://osf.io/83mwk/.},
  keywords = {Education,Open Science,Questionable Research Practices,Replication}
}

@article{Makel2021,
  title = {Both {{Questionable}} and {{Open Research Practices Are Prevalent}} in {{Education Research}}:},
  shorttitle = {Both {{Questionable}} and {{Open Research Practices Are Prevalent}} in {{Education Research}}},
  author = {Makel, Matthew C. and Hodges, Jaret and Cook, Bryan G. and Plucker, Jonathan A.},
  year = {2021},
  month = mar,
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  doi = {10.3102/0013189X211001356},
  abstract = {Concerns about the conduct of research are pervasive in many fields, including education. In this preregistered study, we replicated and extended previous studi...},
  copyright = {\textcopyright{} 2021 AERA},
  journal = {Educational Researcher},
  language = {en}
}

@article{Maxwell2004,
  title = {The {{Persistence}} of {{Underpowered Studies}} in {{Psychological Research}}: {{Causes}}, {{Consequences}}, and {{Remedies}}.},
  shorttitle = {The {{Persistence}} of {{Underpowered Studies}} in {{Psychological Research}}},
  author = {Maxwell, Scott E.},
  year = {2004},
  volume = {9},
  pages = {147--163},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.2.147},
  journal = {Psychological Methods},
  keywords = {meta-science,power,replication crisis,stats},
  language = {en},
  number = {2}
}

@misc{Mitchell2014,
  title = {On the Evidentiary Emptiness of Failed Replications},
  author = {Mitchell, Jason},
  year = {2014},
  url = {https://jasonmitchell.fas.harvard.edu/Papers/Mitchell\_failed\_science\_2014.pdf}
}

@article{Motyl2017,
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  shorttitle = {The State of Social and Personality Science},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  year = {2017},
  volume = {113},
  pages = {34--58},
  issn = {1939-1315, 0022-3514},
  doi = {10.1037/pspa0000084},
  journal = {Journal of Personality and Social Psychology},
  language = {en},
  number = {1}
}

@article{Mueller-Langer2019,
  title = {Replication Studies in Economics\textemdash{{How}} Many and Which Papers Are Chosen for Replication, and Why?},
  author = {{Mueller-Langer}, Frank and Fecher, Benedikt and Harhoff, Dietmar and Wagner, Gert G.},
  year = {2019},
  month = feb,
  volume = {48},
  pages = {62--83},
  issn = {0048-7333},
  doi = {10.1016/j.respol.2018.07.019},
  abstract = {We investigate how often replication studies are published in empirical economics and what types of journal articles are replicated. We find that between 1974 and 2014 0.1\% of publications in the top 50 economics journals were replication studies. We consider the results of published formal replication studies (whether they are negating or reinforcing) and their extent: Narrow replication studies are typically devoted to mere replication of prior work, while scientific replication studies provide a broader analysis. We find evidence that higher-impact articles and articles by authors from leading institutions are more likely to be replicated, whereas the replication probability is lower for articles that appeared in top 5 economics journals. Our analysis also suggests that mandatory data disclosure policies may have a positive effect on the incidence of replication.},
  journal = {Research Policy},
  keywords = {Economic methodology,Economics of science,Replication,Science policy},
  language = {en},
  number = {1}
}

@article{Nosek2014,
  title = {Registered {{Reports}}: {{A Method}} to {{Increase}} the {{Credibility}} of {{Published Results}}},
  shorttitle = {Registered {{Reports}}},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  volume = {45},
  pages = {137--141},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000192},
  journal = {Social Psychology},
  language = {en},
  number = {3}
}

@article{OSC2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  volume = {349},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  abstract = {Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 Structured Abstract INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  journal = {Science},
  language = {en},
  number = {6251},
  pmid = {26315443}
}

@article{Pridemore2018,
  title = {Replication in {{Criminology}} and the {{Social Sciences}}},
  author = {Pridemore, William Alex and Makel, Matthew C. and Plucker, Jonathan A.},
  year = {2018},
  month = jan,
  volume = {1},
  pages = {19--38},
  issn = {2572-4568},
  doi = {10.1146/annurev-criminol-032317-091849},
  abstract = {Replication is a hallmark of science. In recent years, some medical sciences and behavioral sciences struggled with what came to be known as replication crises. As a field, criminology has yet to address formally the threats to our evidence base that might be posed by large-scale and systematic replication attempts, although it is likely we would face challenges similar to those experienced by other disciplines. In this review, we outline the basics of replication, summarize reproducibility problems found in other fields, undertake an original analysis of the amount and nature of replication studies appearing in criminology journals, and consider how criminology can begin to assess more formally the robustness of our knowledge through encouraging a culture of replication.},
  journal = {Annual Review of Criminology},
  keywords = {criminology,meta-science,replication},
  number = {1}
}

@article{Rasmussen2009,
  title = {Association of Trial Registration with the Results and Conclusions of Published Trials of New Oncology Drugs},
  author = {Rasmussen, Nicolas and Lee, Kirby and Bero, Lisa},
  year = {2009},
  month = dec,
  volume = {10},
  pages = {116},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-10-116},
  abstract = {Registration of clinical trials has been introduced largely to reduce bias toward statistically significant results in the trial literature. Doubts remain about whether advance registration alone is an adequate measure to reduce selective publication, selective outcome reporting, and biased design. One of the first areas of medicine in which registration was widely adopted was oncology, although the bulk of registered oncology trials remain unpublished. The net influence of registration on the literature remains untested. This study compares the prevalence of favorable results and conclusions among published reports of registered and unregistered randomized controlled trials of new oncology drugs.},
  journal = {Trials},
  number = {1}
}

@book{Rohatgi2018,
  title = {{{WebPlotDigitizer}} - {{Web Based Plot Digitizer}}},
  author = {Rohatgi, A.},
  year = {2018},
  address = {{Austin, Texas, USA}}
}

@article{Rosenthal1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  author = {Rosenthal, Robert},
  year = {1979},
  volume = {86},
  pages = {638--641},
  issn = {00332909},
  doi = {10.1037/0033-2909.86.3.638},
  abstract = {For any gien research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type 1 errors, while the file drawers are filled with the 95\% of the studies that show non-significant resluts. Quantitative procedures for computing the tolerance for filed and future results are reported and illustrated, and the implications are discussed.},
  journal = {Psychological Bulletin},
  keywords = {tolerance for null results bias in publication of},
  number = {3},
  pmid = {53}
}

@misc{RRRwebsite,
  title = {Registered {{Replication Reports}}},
  abstract = {Quick Links  	Mission Statement  	Article Type Description  	Instructions for Authors  	Instructions for Reviewers  	Ongoing Replication Projects Mission Statement Replicability is a cornerstone of science. Yet replication studies rarely appear in psychology journals. The new Registered Replication Reports \ldots},
  url = {https://www.psychologicalscience.org/publications/replication},
  journal = {Association for Psychological Science - APS},
  language = {en-US}
}

@book{RStudioTeam2019,
  title = {{{RStudio}}: {{Integrated}} Development Environment for r},
  author = {{RStudio Team}},
  year = {2019},
  address = {{Boston, MA}},
  organization = {{RStudio, Inc.}}
}

@article{Simmons2011,
  title = {False-Positive Psychology: {{Undisclosed}} Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  volume = {22},
  pages = {1359--1366},
  issn = {14679280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  journal = {Psychological Science},
  keywords = {disclosure,methodology,motivated reasoning,publication},
  number = {11},
  pmid = {22006061}
}

@article{Simons2014,
  title = {An {{Introduction}} to {{Registered Replication Reports}} at {{{\emph{Perspectives}}}}{\emph{ on }}{{{\emph{Psychological Science}}}}},
  author = {Simons, Daniel J. and Holcombe, Alex O. and Spellman, Barbara A.},
  year = {2014},
  volume = {9},
  pages = {552--555},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691614543974},
  journal = {Perspectives on Psychological Science},
  language = {en},
  number = {5}
}

@article{Simons2018,
  title = {Introducing {{Advances}} in {{Methods}} and {{Practices}} in {{Psychological Science}}},
  author = {Simons, Daniel J.},
  year = {2018},
  month = mar,
  volume = {1},
  pages = {3--6},
  issn = {2515-2459},
  doi = {10.1177/2515245918757424},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {1}
}

@misc{SingletonThorn2019,
  title = {Statistical Power and Effect Sizes in Psychology Are Decreasing over Time},
  author = {Singleton Thorn, Felix and Dudgeon, Paul and Fidler, Fiona},
  year = {2019},
  month = sep,
  address = {{Stanford, CA, USA}},
  keywords = {meta-science,NHST,statistical power},
  language = {en-US}
}

@article{Sterling1959,
  title = {Publication {{Decisions}} and Their {{Possible Effects}} on {{Inferences Drawn}} from {{Tests}} of {{Significance}}\textemdash or {{Vice Versa}}},
  author = {Sterling, Theodore D.},
  year = {1959},
  month = mar,
  volume = {54},
  pages = {30--34},
  issn = {1537274X},
  doi = {10.1080/01621459.1959.10501497},
  abstract = {There is some evidence that in fields where statistical tests of significance are commonly used, research which yields nonsignificant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs-an "error of the first kind"-and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance.},
  journal = {Journal of the American Statistical Association},
  number = {285},
  pmid = {25246403}
}

@article{Sterling1995,
  title = {Publication {{Decisions Revisited}}: {{The Effect}} of the {{Outcome}} of {{Statistical Tests}} on the {{Decision}} to {{Publish}} and {{Vice Versa}}},
  shorttitle = {Publication {{Decisions Revisited}}},
  author = {Sterling, T. D. and Rosenbaum, W. L. and Weinkam, J. J.},
  year = {1995},
  month = feb,
  volume = {49},
  pages = {108},
  issn = {00031305},
  doi = {10.2307/2684823},
  journal = {The American Statistician},
  keywords = {meta-science,NHST,publication bias},
  number = {1}
}

@article{Szucs2017,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  editor = {Wagenmakers, Eric-Jan},
  year = {2017},
  volume = {15},
  pages = {e2000797},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2000797},
  journal = {PLOS Biology},
  language = {en},
  number = {3}
}

@article{vanAssen2014,
  title = {Why {{Publishing Everything Is More Effective}} than {{Selective Publishing}} of {{Statistically Significant Results}}},
  author = {{van Assen}, Marcel A. L. M. and {van Aert}, Robbie C. M. and Nuijten, Mich{\`e}le B. and Wicherts, Jelte M.},
  year = {2014},
  month = jan,
  volume = {9},
  pages = {e84896},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0084896},
  abstract = {Background De Winter and Happee [1] examined whether science based on selective publishing of significant results may be effective in accurate estimation of population effects, and whether this is even more effective than a science in which all results are published (i.e., a science without publication bias). Based on their simulation study they concluded that ``selective publishing yields a more accurate meta-analytic estimation of the true effect than publishing everything, (and that) publishing nonreplicable results while placing null results in the file drawer can be beneficial for the scientific collective'' (p.4). Methods and Findings Using their scenario with a small to medium population effect size, we show that publishing everything is more effective for the scientific collective than selective publishing of significant results. Additionally, we examined a scenario with a null effect, which provides a more dramatic illustration of the superiority of publishing everything over selective publishing. Conclusion Publishing everything is more effective than only reporting significant outcomes.},
  journal = {PLOS ONE},
  keywords = {Meta-analysis,meta-science,Normal distribution,publication bias,Publication ethics,Scientific publishing,Scientists,Simulation and modeling,Social sciences,Statistical methods},
  language = {en},
  number = {1}
}

@article{Veldkamp2018,
  title = {Preprint "{{Ensuring}} the Quality and Specificity of Preregistrations"},
  author = {Veldkamp, Coosje Lisabet Sterre and Bakker, Marjan and {van Assen}, Marcel A.L.M. and Crompvoets, Elise Anne Victoire and Ong, How Hwee and Soderberg, Courtney K. and Mellor, David and Nosek, Brian A. and Wicherts, Jelte},
  year = {2018},
  pages = {1--30},
  doi = {10.31234/OSF.IO/CDGYH}
}

@article{Wilkinson1999,
  title = {Statistical Methods in Psychology Journals: {{Guidelines}} and Explanations},
  author = {Wilkinson, Leland},
  year = {1999},
  volume = {54},
  pages = {594--604},
  issn = {0003066X},
  doi = {10.1037/0003-066X.54.8.594},
  abstract = {In the light of continuing debate over the applications of significance testing in psychology journals and following the publication of J. Cohen's (1994) article, the Board of Scientific Affairs (BSA) of the American Psychological Association (APA) convened a committee called the Task Force on Statistical Interference (TFSI) whose charge was "to elucidate some of the controversial issues surrounding applications of statistics including significance testing and its alternatives; alternative underlying models and data transformation; and newer methods made possible by powerful computers" (BSA, personal communication, February 28, 1996). After extensive discussion, the BSA recommended that publishing an article in American Psychologist, as a way to initiate discussion in the field about changes in current practices of data analysis and reporting may be appropriate. This report follows that request. Following each guideline are comments, explanations, or elaborations assembled by L. Wilkinson for the task force and under its review. The report is concerned with the use of statistical methods only and is not meant as an assessment of research methods in general. The title and format of the report are adapted from an article by J. C. Bailar and F. Mosteller (1988).},
  journal = {American Psychologist},
  number = {8},
  pmid = {18793039}
}

@article{Wiseman2019,
  title = {Registered Reports: An Early Example and Analysis},
  shorttitle = {Registered Reports},
  author = {Wiseman, Richard and Watt, Caroline and Kornbrot, Diana},
  year = {2019},
  month = jan,
  volume = {7},
  pages = {e6232},
  issn = {2167-8359},
  doi = {10.7717/peerj.6232},
  abstract = {The recent `replication crisis' in psychology has focused attention on ways of increasing methodological rigor within the behavioral sciences. Part of this work has involved promoting `Registered Reports', wherein journals peer review papers prior to data collection and publication. Although this approach is usually seen as a relatively recent development, we note that a prototype of this publishing model was initiated in the mid-1970s by parapsychologist Martin Johnson in the European Journal of Parapsychology (EJP). A retrospective and observational comparison of Registered and non-Registered Reports published in the EJP during a seventeen-year period provides circumstantial evidence to suggest that the approach helped to reduce questionable research practices. This paper aims both to bring Johnson's pioneering work to a wider audience, and to investigate the positive role that Registered Reports may play in helping to promote higher methodological and statistical standards.},
  journal = {PeerJ},
  keywords = {history of science,meta-science,parapsychology,RRs},
  language = {en}
}

@misc{zotero-17647,
  title = {(1) {{Brian Nosek}} on {{Twitter}}: "{{For}} Example, the First Time Preregistering an Analysis Plan, Many People Report Being Shocked at How Hard It Is without Seeing the Data. {{It}} Produces a Recognition That Our Analysis Decision-Making (and Hypothesizing) Had Been Much More Data Contingent than We Realized. 2/" / {{Twitter}}},
  shorttitle = {(1) {{Brian Nosek}} on {{Twitter}}},
  url = {https://twitter.com/BrianNosek/status/1132239170729127936},
  journal = {Twitter},
  language = {en-GB}
}


